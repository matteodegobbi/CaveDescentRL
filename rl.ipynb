{
 "cells": [
  {
   "cell_type": "code",
   "id": "25856370-b814-4fb7-9262-7adbe54c1e92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T23:25:06.202094Z",
     "start_time": "2025-05-02T23:25:06.125134Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2a14f42f-8e3f-4b9a-b498-9fefa3993681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T23:26:37.956217Z",
     "start_time": "2025-05-02T23:26:37.943533Z"
    }
   },
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.board = np.full((3, 3), -1)\n",
    "        self.current_player = 0\n",
    "        self.observation_space = 9\n",
    "        self.action_space = 9\n",
    "    \n",
    "    def step(self, move: int):\n",
    "        self.make_move(move, self.current_player)\n",
    "\n",
    "        if self.is_done():\n",
    "            # Current player won\n",
    "            return self.board, 1, True, False, {}\n",
    "\n",
    "        self.current_player = self.get_opposite_player()\n",
    "        other_move = self.get_random_move()\n",
    "        if other_move is None:\n",
    "            # Draw\n",
    "            return self.board, 0, True, False, {}\n",
    "        self.make_move(other_move, self.current_player)\n",
    "\n",
    "        if self.is_done():\n",
    "            # The other player won\n",
    "            return self.board, -1, True, False, {}\n",
    "\n",
    "        self.current_player = self.get_opposite_player()\n",
    "        # next_state, reward, terminated, truncated, _\n",
    "        return self.board, 0, False, False, {}\n",
    "\n",
    "    def make_move(self, move, player):\n",
    "        one_hot_move = np.zeros(self.action_space)\n",
    "        one_hot_move[move] = 1\n",
    "        coords = (move // 3, move % 3)\n",
    "        if self.board[coords] != -1:\n",
    "            raise Exception(\"Already occupied\")\n",
    "        self.board[coords] = player\n",
    "\n",
    "    def get_random_move(self):\n",
    "        available_coords = list(zip(*np.where(self.board == -1)))\n",
    "        available = [r * 3 + c for r, c in available_coords]\n",
    "        if not available:\n",
    "            return None\n",
    "        return random.choice(available)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.full((3, 3), -1)\n",
    "        self.current_player = 0\n",
    "        return self.board\n",
    "\n",
    "    def get_opposite_player(self):\n",
    "        return 1 - self.current_player\n",
    "        \n",
    "    def is_done(self):\n",
    "        for i in range(3):\n",
    "            if np.all(self.board[i, :] == self.current_player):\n",
    "                return True\n",
    "            if np.all(self.board[:, i] == self.current_player):\n",
    "                return True\n",
    "        if np.all(np.diag(self.board) == self.current_player):\n",
    "            return True\n",
    "        if np.all(np.diag(np.fliplr(self.board)) == self.current_player):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def sample_action(self):\n",
    "        return self.get_random_move()\n",
    "        \n",
    "    def print_board(self):\n",
    "        for row in self.board:\n",
    "            print(\"|\".join(self.symbol(cell) for cell in row))\n",
    "            print(\"-\" * 5)\n",
    "            \n",
    "    def symbol(self,val):\n",
    "        if val == 0:\n",
    "            return \"X\"\n",
    "        elif val == 1:\n",
    "            return \"O\"\n",
    "        else:\n",
    "            return \" \""
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "cbf90d4e-0859-424a-854e-38692d805251",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-02T23:25:39.045122Z",
     "start_time": "2025-05-02T23:25:39.040051Z"
    }
   },
   "source": [
    "env = Environment()\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "env.print_board()\n",
    "while not done:\n",
    "    action = env.sample_action()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    env.print_board()\n",
    "    print(f\"Reward: {reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " | | \n",
      "-----\n",
      " |O|X\n",
      "-----\n",
      "Reward: 0\n",
      " | |X\n",
      "-----\n",
      " | |O\n",
      "-----\n",
      " |O|X\n",
      "-----\n",
      "Reward: 0\n",
      " |X|X\n",
      "-----\n",
      " |O|O\n",
      "-----\n",
      " |O|X\n",
      "-----\n",
      "Reward: 0\n",
      "X|X|X\n",
      "-----\n",
      " |O|O\n",
      "-----\n",
      " |O|X\n",
      "-----\n",
      "Reward: 1\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "9746e7df-22cc-496c-add7-aa00db0d6552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T23:30:11.320568Z",
     "start_time": "2025-05-02T23:30:11.013427Z"
    }
   },
   "source": [
    "# Set random seed\n",
    "# np.random.seed()\n",
    "\n",
    "# Variable creation and initialization\n",
    "Q = np.zeros((env.observation_space, env.action_space))\n",
    "\n",
    "training = True\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "gamma = 0.5\n",
    "done = False\n",
    "\n",
    "rewards = []\n",
    "episodes_count = 0\n",
    "\n",
    "while training:\n",
    "    # Perform episode\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward_sum = 0\n",
    "    while not done:\n",
    "        # if np.random.rand() < epsilon:\n",
    "        action = env.sample_action()\n",
    "        # else:\n",
    "            # action = np.argmax(Q[state])\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update the action-value estimates\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "        # env.print_board()\n",
    "        # print(action, reward)\n",
    "\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    rewards.append(reward_sum)\n",
    "    episodes_count += 1\n",
    "\n",
    "    if episodes_count % 10 == 0:\n",
    "        mean_return = np.mean(rewards[-100:])\n",
    "        std_return = np.std(rewards[-100:])\n",
    "        recent_returns = rewards[-10:]\n",
    "        returns_str = \" \".join(map(str, recent_returns))\n",
    "        print(\n",
    "            f\"Episode {episodes_count}, mean 100-episode return {mean_return:.2f} +-{std_return:.2f}, returns {returns_str}\")\n",
    "\n",
    "    if episodes_count > 1000:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, mean 100-episode return 0.70 +-0.46, returns 1 1 1 1 1 1 1 0 0 0\n",
      "Episode 20, mean 100-episode return 0.60 +-0.66, returns 1 1 -1 1 1 -1 1 1 1 0\n",
      "Episode 30, mean 100-episode return 0.33 +-0.83, returns 1 0 -1 0 1 -1 -1 -1 -1 1\n",
      "Episode 40, mean 100-episode return 0.38 +-0.80, returns 0 1 1 1 -1 0 1 0 1 1\n",
      "Episode 50, mean 100-episode return 0.36 +-0.82, returns -1 1 1 1 -1 1 -1 1 0 1\n",
      "Episode 60, mean 100-episode return 0.28 +-0.84, returns -1 0 1 -1 -1 0 1 -1 1 0\n",
      "Episode 70, mean 100-episode return 0.27 +-0.84, returns 1 0 -1 1 -1 1 0 1 1 -1\n",
      "Episode 80, mean 100-episode return 0.26 +-0.86, returns 1 1 1 1 1 1 -1 -1 -1 -1\n",
      "Episode 90, mean 100-episode return 0.21 +-0.89, returns -1 -1 1 1 1 -1 -1 1 -1 -1\n",
      "Episode 100, mean 100-episode return 0.27 +-0.88, returns 1 1 1 1 1 -1 1 1 1 1\n",
      "Episode 110, mean 100-episode return 0.27 +-0.89, returns 1 1 1 1 1 1 1 -1 1 0\n",
      "Episode 120, mean 100-episode return 0.20 +-0.92, returns -1 -1 -1 1 -1 -1 1 1 -1 1\n",
      "Episode 130, mean 100-episode return 0.23 +-0.91, returns -1 -1 1 -1 1 0 1 1 1 -1\n",
      "Episode 140, mean 100-episode return 0.17 +-0.93, returns 0 -1 1 -1 -1 0 1 0 -1 1\n",
      "Episode 150, mean 100-episode return 0.19 +-0.91, returns 0 1 1 0 0 1 1 -1 1 1\n",
      "Episode 160, mean 100-episode return 0.26 +-0.91, returns -1 1 1 1 1 -1 1 1 1 1\n",
      "Episode 170, mean 100-episode return 0.24 +-0.93, returns 1 1 1 1 -1 -1 -1 1 -1 -1\n",
      "Episode 180, mean 100-episode return 0.26 +-0.91, returns 1 -1 1 -1 1 1 0 1 1 0\n",
      "Episode 190, mean 100-episode return 0.31 +-0.88, returns 0 -1 1 1 1 -1 1 1 0 0\n",
      "Episode 200, mean 100-episode return 0.26 +-0.89, returns -1 1 1 1 -1 1 1 1 -1 0\n",
      "Episode 210, mean 100-episode return 0.25 +-0.89, returns 1 0 -1 1 1 1 1 0 1 1\n",
      "Episode 220, mean 100-episode return 0.29 +-0.88, returns 1 -1 1 1 1 1 -1 -1 -1 1\n",
      "Episode 230, mean 100-episode return 0.28 +-0.88, returns -1 1 1 -1 -1 1 1 -1 1 -1\n",
      "Episode 240, mean 100-episode return 0.34 +-0.87, returns 0 1 1 -1 1 1 -1 1 1 1\n",
      "Episode 250, mean 100-episode return 0.28 +-0.91, returns -1 0 -1 -1 1 1 1 1 -1 -1\n",
      "Episode 260, mean 100-episode return 0.25 +-0.91, returns -1 1 1 0 1 1 1 -1 1 -1\n",
      "Episode 270, mean 100-episode return 0.27 +-0.90, returns -1 1 1 1 1 -1 -1 -1 1 1\n",
      "Episode 280, mean 100-episode return 0.24 +-0.92, returns -1 1 1 1 0 -1 -1 1 1 -1\n",
      "Episode 290, mean 100-episode return 0.24 +-0.93, returns 1 1 -1 1 -1 -1 1 0 1 1\n",
      "Episode 300, mean 100-episode return 0.26 +-0.92, returns 1 1 1 1 -1 0 1 1 1 -1\n",
      "Episode 310, mean 100-episode return 0.24 +-0.93, returns 1 1 1 0 1 -1 1 -1 1 0\n",
      "Episode 320, mean 100-episode return 0.26 +-0.92, returns 1 1 -1 1 1 1 -1 1 -1 1\n",
      "Episode 330, mean 100-episode return 0.30 +-0.90, returns -1 1 1 1 0 -1 0 1 1 1\n",
      "Episode 340, mean 100-episode return 0.29 +-0.90, returns 1 1 1 0 0 -1 -1 1 1 1\n",
      "Episode 350, mean 100-episode return 0.39 +-0.86, returns 1 1 1 1 1 1 1 0 1 1\n",
      "Episode 360, mean 100-episode return 0.38 +-0.86, returns 1 0 1 0 -1 -1 1 -1 1 1\n",
      "Episode 370, mean 100-episode return 0.36 +-0.87, returns -1 -1 -1 1 1 1 -1 1 -1 1\n",
      "Episode 380, mean 100-episode return 0.32 +-0.88, returns -1 -1 0 -1 -1 -1 -1 1 1 1\n",
      "Episode 390, mean 100-episode return 0.31 +-0.89, returns -1 -1 -1 1 1 1 1 1 -1 1\n",
      "Episode 400, mean 100-episode return 0.26 +-0.91, returns -1 1 -1 1 -1 -1 1 1 -1 1\n",
      "Episode 410, mean 100-episode return 0.23 +-0.93, returns 1 0 1 -1 -1 -1 1 -1 1 1\n",
      "Episode 420, mean 100-episode return 0.21 +-0.93, returns 1 1 -1 1 -1 -1 -1 1 1 1\n",
      "Episode 430, mean 100-episode return 0.26 +-0.92, returns 0 1 1 1 1 1 1 1 1 1\n",
      "Episode 440, mean 100-episode return 0.24 +-0.94, returns -1 1 1 1 1 -1 -1 -1 1 1\n",
      "Episode 450, mean 100-episode return 0.22 +-0.94, returns 1 1 1 1 1 1 1 1 0 -1\n",
      "Episode 460, mean 100-episode return 0.27 +-0.94, returns -1 1 1 1 0 1 1 1 1 1\n",
      "Episode 470, mean 100-episode return 0.29 +-0.92, returns 1 -1 1 1 1 1 -1 -1 0 0\n",
      "Episode 480, mean 100-episode return 0.38 +-0.89, returns -1 1 1 1 1 1 1 1 -1 1\n",
      "Episode 490, mean 100-episode return 0.43 +-0.86, returns 1 1 1 1 1 1 1 1 -1 0\n",
      "Episode 500, mean 100-episode return 0.45 +-0.85, returns -1 -1 1 1 -1 1 1 1 1 -1\n",
      "Episode 510, mean 100-episode return 0.45 +-0.84, returns 0 -1 -1 -1 1 1 1 0 1 0\n",
      "Episode 520, mean 100-episode return 0.48 +-0.81, returns 0 1 0 0 1 1 -1 1 1 1\n",
      "Episode 530, mean 100-episode return 0.43 +-0.84, returns 1 1 -1 -1 1 1 1 1 -1 1\n",
      "Episode 540, mean 100-episode return 0.47 +-0.82, returns 1 -1 1 1 1 1 1 1 -1 1\n",
      "Episode 550, mean 100-episode return 0.43 +-0.83, returns -1 1 0 0 1 -1 0 1 1 1\n",
      "Episode 560, mean 100-episode return 0.41 +-0.84, returns 1 1 1 1 -1 -1 1 1 1 0\n",
      "Episode 570, mean 100-episode return 0.45 +-0.83, returns -1 -1 1 1 1 1 1 1 1 1\n",
      "Episode 580, mean 100-episode return 0.41 +-0.84, returns -1 1 0 1 1 -1 1 -1 0 1\n",
      "Episode 590, mean 100-episode return 0.37 +-0.86, returns -1 1 1 1 1 -1 0 -1 1 1\n",
      "Episode 600, mean 100-episode return 0.34 +-0.85, returns 1 -1 0 -1 1 -1 1 -1 0 0\n",
      "Episode 610, mean 100-episode return 0.33 +-0.87, returns 1 1 -1 -1 -1 -1 1 1 -1 1\n",
      "Episode 620, mean 100-episode return 0.26 +-0.90, returns -1 -1 1 1 0 0 -1 -1 1 -1\n",
      "Episode 630, mean 100-episode return 0.26 +-0.90, returns 1 1 1 1 1 -1 -1 1 1 -1\n",
      "Episode 640, mean 100-episode return 0.21 +-0.91, returns 1 1 0 -1 1 1 -1 1 -1 -1\n",
      "Episode 650, mean 100-episode return 0.21 +-0.92, returns 1 0 1 1 1 1 1 -1 -1 -1\n",
      "Episode 660, mean 100-episode return 0.20 +-0.93, returns 1 1 1 -1 1 1 -1 1 -1 1\n",
      "Episode 670, mean 100-episode return 0.21 +-0.92, returns 0 1 -1 1 1 1 1 1 1 1\n",
      "Episode 680, mean 100-episode return 0.24 +-0.92, returns -1 1 1 1 1 1 1 0 -1 1\n",
      "Episode 690, mean 100-episode return 0.26 +-0.91, returns 1 1 0 1 1 1 1 -1 1 -1\n",
      "Episode 700, mean 100-episode return 0.31 +-0.89, returns 1 1 0 0 1 1 -1 0 0 1\n",
      "Episode 710, mean 100-episode return 0.37 +-0.87, returns -1 1 1 1 -1 1 1 1 1 1\n",
      "Episode 720, mean 100-episode return 0.42 +-0.85, returns 1 -1 1 1 1 1 -1 1 -1 0\n",
      "Episode 730, mean 100-episode return 0.39 +-0.86, returns 1 -1 -1 1 1 -1 1 -1 1 0\n",
      "Episode 740, mean 100-episode return 0.46 +-0.83, returns 1 1 1 -1 1 1 1 1 1 1\n",
      "Episode 750, mean 100-episode return 0.48 +-0.81, returns 0 0 0 1 1 1 1 1 1 -1\n",
      "Episode 760, mean 100-episode return 0.47 +-0.79, returns 0 1 1 1 0 -1 -1 1 0 1\n",
      "Episode 770, mean 100-episode return 0.50 +-0.78, returns 1 1 1 1 1 1 1 1 1 1\n",
      "Episode 780, mean 100-episode return 0.49 +-0.78, returns 1 1 -1 -1 1 1 0 0 1 1\n",
      "Episode 790, mean 100-episode return 0.46 +-0.81, returns -1 1 1 1 1 -1 1 -1 1 -1\n",
      "Episode 800, mean 100-episode return 0.47 +-0.81, returns 0 0 1 1 1 0 1 -1 1 1\n",
      "Episode 810, mean 100-episode return 0.46 +-0.81, returns 1 -1 1 1 1 1 -1 1 1 0\n",
      "Episode 820, mean 100-episode return 0.45 +-0.82, returns 1 -1 1 -1 -1 1 1 1 1 -1\n",
      "Episode 830, mean 100-episode return 0.52 +-0.78, returns -1 1 1 1 1 1 1 1 1 1\n",
      "Episode 840, mean 100-episode return 0.48 +-0.78, returns 0 1 1 -1 0 0 1 1 0 1\n",
      "Episode 850, mean 100-episode return 0.50 +-0.78, returns 1 1 1 -1 1 1 1 0 1 1\n",
      "Episode 860, mean 100-episode return 0.51 +-0.78, returns -1 0 1 1 1 0 -1 1 1 1\n",
      "Episode 870, mean 100-episode return 0.40 +-0.84, returns -1 0 1 1 1 1 -1 -1 -1 -1\n",
      "Episode 880, mean 100-episode return 0.37 +-0.86, returns 1 1 0 1 -1 -1 -1 1 1 -1\n",
      "Episode 890, mean 100-episode return 0.41 +-0.83, returns 1 1 1 -1 1 0 1 1 1 0\n",
      "Episode 900, mean 100-episode return 0.41 +-0.84, returns 1 -1 1 0 -1 1 1 1 1 1\n",
      "Episode 910, mean 100-episode return 0.40 +-0.85, returns 1 1 1 1 1 -1 -1 1 1 -1\n",
      "Episode 920, mean 100-episode return 0.40 +-0.85, returns -1 1 -1 1 1 1 -1 1 -1 1\n",
      "Episode 930, mean 100-episode return 0.33 +-0.87, returns -1 1 1 -1 1 -1 1 1 -1 0\n",
      "Episode 940, mean 100-episode return 0.34 +-0.89, returns 1 1 1 -1 1 1 1 -1 0 1\n",
      "Episode 950, mean 100-episode return 0.31 +-0.89, returns 0 1 1 -1 1 -1 1 0 1 1\n",
      "Episode 960, mean 100-episode return 0.29 +-0.90, returns 1 1 0 -1 1 1 -1 -1 0 1\n",
      "Episode 970, mean 100-episode return 0.34 +-0.89, returns 1 1 1 1 1 1 1 -1 -1 -1\n",
      "Episode 980, mean 100-episode return 0.38 +-0.87, returns -1 1 1 1 1 1 1 1 -1 0\n",
      "Episode 990, mean 100-episode return 0.35 +-0.89, returns -1 -1 1 1 1 1 1 1 -1 0\n",
      "Episode 1000, mean 100-episode return 0.35 +-0.89, returns 1 1 1 1 1 0 -1 1 1 -1\n"
     ]
    }
   ],
   "execution_count": 61
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
